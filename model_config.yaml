hydra:
  job_logging:
    root:
      handlers: [file, console]  # logging to file only.
  run:
    dir: ./



# ****************** Common Configurations *******************************
model_name_or_path: EleutherAI/gpt-j-6B 
log_dir: performance # Performance dir
few_dset_dir: revisiting_few # fewshot_raw_dset dir
shots: 4 # 4, 8, full
seed: 915 # 13, 27, 250, 583, 915

### Methods
### baseline -> apply_prompt: False, demonstration: False
### PALP-T -> apply_prompt: True, demonstration: False
### PALP-D -> apply_prompt: True, demonstration: True
apply_prompt: True
demonstrations: True
# demonstrations: False
max_demonstrations: 20
# ************************************************************************



# ****************** Representation Extractions ***************************
extract_tasks: {sst2: glue} # Available tasks -> {sst2: glue, rotten_tomatoes: huggingface, offensive: tweet_eval, cola: glue, stance_atheism: tweet_eval, emotion: tweet_eval, ag_news: huggingface, trec: huggingface, banking77: huggingface, plus: clinc_oos, mnli: glue,mrpc: glue,rte: glue,boolq: super_glue,cb: super_glue}
output_dir: ./output
extraction_batch_size: 10 # Please set this hyper-parameter considering your GPU Memory.
# ************************************************************************



# ****************** Linear Probing Evaluation ***************************
task_name: sst2 # choices
nn_batch_size: 2
nn_early_stop: 150
nn_lr: 1e-3

# Gaussian eval config  
dset: full # choices = [fewshot, full]
demon_dir: final_demon
shot_num: 100 # for few-shot evaluation
# ************************************************************************
